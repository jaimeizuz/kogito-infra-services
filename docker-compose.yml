services:
  kogito-bpmn-processes:
    container_name: kogito-bpmn-processes
    image: dev.local/kogito/kogito-bpmn-processes:0.0.1-SNAPSHOT
    profiles:
      - kogito-bpmn-processes
    ports:
      - '8080:8080'
      - '8083:8083'
    cpus: 1
    mem_limit: 500m
    environment:
      KOGITO_PERSISTENCE_PROTO_MARSHALLER: "false"
      QUARKUS_HTTP_CORS_ORIGINS: /.*/
      # See https://quarkus.io/guides/datasource#jdbc-configuration
      QUARKUS_DATASOURCE_JDBC_URL: 'jdbc:postgresql://postgres:5432/kogito?currentSchema=public'
      QUARKUS_DATASOURCE_REACTIVE_URL: 'postgresql://postgres:5432/kogito?currentSchema=public'
      QUARKUS_DATASOURCE_USERNAME: kogito-user
      QUARKUS_DATASOURCE_PASSWORD: kogito-pass
      QUARKUS_DATASOURCE_DB_KIND: postgresql
      QUARKUS_DATASOURCE_JDBC_ENABLE_METRICS: "false"
      QUARKUS_DATASOURCE_JDBC_METRICS_ENABLED: "false"
      # The initial size of the pool. Usually you will want to set the initial size to match at least the minimal size, 
      # but this is not enforced so to allow for architectures which prefer a lazy initialization of the connections on boot, 
      # while being able to sustain a minimal pool size after boot. DEFAULT: null
      QUARKUS_DATASOURCE_JDBC_INITIAL_SIZE: 20
      # The datasource pool minimum size. DEFAULT: 0
      QUARKUS_DATASOURCE_JDBC_MIN_SIZE: 20
      # The datasource pool maximum size. DEFAULT: 20
      QUARKUS_DATASOURCE_JDBC_MAX_SIZE: 40
      # The interval at which we validate idle connections in the background. Set to 0 to disable background validation. DEFAULT: 2M
      # QUARKUS_DATASOURCE_JDBC_BACKGROUND_VALIDATION_INTERVAL: 2M
      # Perform foreground validation on connections that have been idle for longer than the specified interval. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_FOREGROUND_VALIDATION_INTERVAL: 15S
      # The timeout before cancelling the acquisition of a new connection. DEFAULT: 5S
      # QUARKUS_DATASOURCE_JDBC_ACQUISITION_TIMEOUT: 5S
      # The interval at which we check for connection leaks. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_LEAK_DETECTION_INTERVAL: 10M
      # The interval at which we try to remove idle connections. DEFAULT: 5M
      # QUARKUS_DATASOURCE_JDBC_IDLE_REMOVAL_INTERVAL: 5M
      # The max lifetime of a connection. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_MAX_LIFETIME: 1M
      # The transaction isolation level. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_TRANSACTION_ISOLATION_LEVEL: undefined, none, read-uncommitted, read-committed, repeatable-read, serializable
      KOGITO_SERVICE_URL: 'http://kogito-bpmn-processes:8080'
      KOGITO_DATA_INDEX_URL: 'http://localhost:8080'
      KOGITO_JOBS_SERVICE_URL: 'http://localhost:8080'
      # The current chunk size in minutes the scheduler handles, it is used to keep a limit number of jobs scheduled
      # in the in-memory scheduler.
      #KOGITO_JOBS-SERVICE_SCHEDULERCHUNKINMINUTES: 10
      # The interval the job loading method runs to fetch the persisted jobs from the repository.
      #KOGITO_JOBS-SERVICE_LOADJOBINTERVALINMINUTES: 10
      # The interval based on the current time the job loading method uses to fetch jobs "FROM (now -
      # {@link #loadJobFromCurrentTimeIntervalInMinutes}) TO {@link #schedulerChunkInMinutes}"
      #KOGITO_JOBS-SERVICE_LOADJOBFROMCURRENTTIMEINTERVALINMINUTES: 0
      # Number of retries configured for the periodic jobs loading procedure. Every time the procedure is started this
      # value is considered.
      KOGITO_JOBS-SERVICE_LOADJOBRETRIES: 3
      # Error strategy to apply when the periodic jobs loading procedure has exceeded the jobLoadReties. NONE, FAIL_SERVICE
      #KOGITO_JOBS-SERVICE_LOADJOBERRORSTRATEGY: FAIL_SERVICE
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBS: "true"
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBSONSERVICESTART: "true"
      QUARKUS_OTEL_LOGS_ENABLED: "true"
      QUARKUS_OTEL_EXPORTER_OTLP_LOGS_ENDPOINT: http://opentelemetry-collector:4317
      QUARKUS_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT: http://opentelemetry-collector:4317
      #QUARKUS_MICROMETER_EXPORT_OTLP_URL: http://opentelemetry-collector:4318
      QUARKUS_DATASOURCE_JDBC_TELEMETRY: "true"
      QUARKUS_DATASOURCE_JDBC_TELEMETRY_ENABLED: "true"
      QUARKUS_DATASOURCE_JDBC_TRACING: "true"
      QUARKUS_DATASOURCE_JDBC_TRACING_ENABLED: "true"
      QUARKUS_LOG_HANDLER_GELF_ENABLED: "true"
      QUARKUS_LOG_HANDLER_GELF_HOST: logstash
      QUARKUS_LOG_HANDLER_GELF_PORT: 12201
      QUARKUS_LOG_HANDLER_GELF_INCLUDE_FULL_MDC: "true"
      KAFKA_BOOTSTRAP_SERVERS: redpanda:9093
      #MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_TOPIC: kogito-processinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      #MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_TOPIC: kogito-usertaskinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      #MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_TOPIC: kogito-processdefinitions-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      #KOGITO_ADDON_MESSAGING_INCOMING_DEFAULTNAME: kogito-app-example-incoming-stream
      #MP_MESSAGING_INCOMING_KOGITO-INCOMING-STREAM_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_INCOMING_KOGITO-INCOMING-STREAM_CONNECTOR: smallrye-kafka
      MP_MESSAGING_INCOMING_KOGITO-INCOMING-STREAM_TOPIC: sendConfirmationForm
      MP_MESSAGING_INCOMING_KOGITO-INCOMING-STREAM_VALUE_DESERIALIZER: org.apache.kafka.common.serialization.StringDeserializer
      #KOGITO_ADDON_MESSAGING_OUTGOING_DEFAULTNAME: kogito-app-example-outgoing-stream
      #MP_MESSAGING_OUTGOING_KOGITO-OUTGOING-STREAM_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_OUTGOING_KOGITO-OUTGOING-STREAM_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-OUTGOING-STREAM_TOPIC: receiveConfirmation
      MP_MESSAGING_OUTGOING_KOGITO-OUTGOING-STREAM_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      #KOGITO_ADDON_MONITORING_ELASTIC_HOST: http://elasticsearch:9200
      #KOGITO_ADDON_MONITORING_ELASTIC_INDEX: kogito-bpmn-processes
      #KOGITO_ADDON_MONITORING_ELASTIC_AUTOCREATEINDEX: "false"
      #KOGITO_ADDON_MONITORING_ELASTIC_STEP: 3s
      KOGITO_EVENTS_GROUPING: "false"
      KIE_FLYWAY_ENABLED: "true"
      KOGITO_PROCESSES_TRANSACTIONENABLED: "true"
      KOGITO_USERTASKS_TRANSACTIONENABLED: "true"
      QUARKUS_REST_CLIENT__DUMMY_REST_SERVICE__URL: http://wiremock-studio:9002
    depends_on:
      postgres:
        condition: service_started
      redpanda:
        condition: service_healthy
  bamoe-maven-repository:
    container_name: bamoe-maven-repository
    #image: apache/incubator-kie-kogito-management-console:10.0.0
    image: jizuzquiza/bamoe-maven-repository:9.1.2-ibm-0006
    profiles:
      - full
    ports:
      - '8800:8080'
  kogito-management-console:
    container_name: kogito-management-console
    image: apache/incubator-kie-kogito-management-console:main
    #image: jizuzquiza/bamoe-management-console:9.1.2-ibm-0006
    profiles:
      - full
      - kogito-consoles
    ports:
      - '8280:8080'
    #depends_on:
    #  keycloak:
    #    condition: service_healthy
    environment:
      KOGITO_CONSOLES_KEYCLOAK_HEALTH_CHECK_URL: >-
        http://localhost:8480/realms/kogito/.well-known/openid-configuration
      KOGITO_CONSOLES_KEYCLOAK_URL: 'http://localhost:8480'
      KOGITO_CONSOLES_KEYCLOAK_REALM: kogito
      KOGITO_CONSOLES_KEYCLOAK_CLIENT_ID: kogito-console-quarkus
      RUNTIME_TOOLS_MANAGEMENT_CONSOLE_KOGITO_ENV_MODE: "PROD"
      RUNTIME_TOOLS_MANAGEMENT_CONSOLE_DATA_INDEX_ENDPOINT: http://localhost:8080/graphql
      RUNTIME_TOOLS_TASK_CONSOLE_KOGITO_TASK_STATES_LIST: "Active,Ready,Reserved,Completed,Aborted,Skipped"
      RUNTIME_TOOLS_TASK_CONSOLE_KOGITO_TASK_ACTIVE_STATES_LIST: "Active,Ready,Reserved"
  postgres:
    container_name: postgres
    image: 'postgres:16.1-alpine3.19'
    profiles:
      - infra
      - example
      - full
      - kogito-database
    ports:
      - '5432:5432'
    volumes:
      - './sql:/docker-entrypoint-initdb.d:Z'
    healthcheck:
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - kogito
        - '-U'
        - kogito-user
      timeout: 45s
      interval: 10s
      retries: 50
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  pgadmin:
    image: dpage/pgadmin4:8.2
    container_name: pgadmin
    profiles:
      - full
      - pgadmin
    ports:
      - 8055:80
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json
      - ./pgadmin/pgpass:/pgadmin4/pgpass
    entrypoint: >
      /bin/sh -c "
      cp -f /pgadmin4/pgpass /var/lib/pgadmin/;
      chmod 600 /var/lib/pgadmin/pgpass;
      /entrypoint.sh
      "
    environment:
      PGADMIN_DEFAULT_EMAIL: user@kogito.org
      PGADMIN_DEFAULT_PASSWORD: pass
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
      GUNICORN_ACCESS_LOGFILE: "/dev/null"
  mysql-server:
    container_name: mysql
    image: jizuzquiza/mysql:8.0.16
    profiles:
      - full
      - business-services
    environment:
      MYSQL_ROOT_PASSWORD: password@1
    volumes:
      - mysql_data:/var/lib/mysql
    ports:
    - 3307:3306
  keycloak:
    container_name: keycloak
    #image: quay.io/keycloak/keycloak:23.0.7-0
    image: quay.io/keycloak/keycloak:26.1.0
    profiles:
      - full
      - kogito-consoles
    ports:
      - '8480:8080'
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      #- ./keycloak/kogito-realm.json:/tmp/kogito-realm.json
      - ./keycloak:/opt/keycloak/data/import:z
    healthcheck:
      test:
        - CMD
        - curl
        - '-f'
        - 'http://localhost:8080/realms/kogito'
      interval: 2s
      timeout: 1s
      retries: 50
    environment:
      DB_VENDOR: POSTGRES
      DB_ADDR: postgresql
      DB_DATABASE: keycloak
      DB_USER: kogito-user
      DB_SCHEMA: public
      DB_PASSWORD: kogito-pass
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    command:
      start-dev --import-realm --hostname-port=8480 --health-enabled=true
  redpanda:
    container_name: redpanda
    image: redpandadata/redpanda:v24.2.14
    profiles:
      - full
      - messaging-redpanda
    ports:
      - 18081:18081
      - 18082:18082
      - 14001:8081
      - 14002:8082
      - 9092:9092
      - 28082:28082
      - 29092:2909
      - 9093:9093
      - 9644:9644
    volumes:
      - redpanda_data:/var/lib/redpanda/data
      #- ./redpanda/bootstrap.yml:/etc/redpanda/.bootstrap.yaml
    #environment:
      #DATA_TRANSFORMS_ENABLED: "true"
    command:
    - redpanda
    - start
    - --smp
    - '6'
    - --reserve-memory
    - 0M
    - --overprovisioned
    - --node-id
    - '0'
    - --kafka-addr
    - PLAINTEXT://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
    - --advertise-kafka-addr
    - PLAINTEXT://redpanda:9093,OUTSIDE://localhost:9092
    - --pandaproxy-addr
    - PLAINTEXT://0.0.0.0:28082,OUTSIDE://0.0.0.0:8082
    - --advertise-pandaproxy-addr
    - PLAINTEXT://redpanda:28082,OUTSIDE://localhost:8082
    - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8082/brokers" ]
      interval: 1s
      timeout: 1s
      retries: 50
  redpanda-connect:
    # See https://docs.redpanda.com/current/console/quickstart/
    container_name: redpanda-connect
    image: redpandadata/connect:4.44.0
    depends_on:
      - redpanda
    profiles:
      - messaging-redpanda-connect
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: redpanda:9093
      CONNECT_REST_ADVERTISED_HOST_NAME: redpanda-connect
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      #CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://redpanda:8081
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8082/brokers" ]
      interval: 1s
      timeout: 1s
      retries: 50
  redpanda-console:
    container_name: redpanda-console
    image: redpandadata/console:v2.8.1
    profiles:
      - full
      - messaging-redpanda
    # mount the local directory that contains your license key to the container.
    # give Redpanda Console read access to the license.
    volumes:
      - ./license:/etc/redpanda:ro
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml && echo "$$CONSOLE_ROLEBINDINGS_CONFIG_FILE" > /tmp/role-bindings.yml && /app/console'
    environment:
      #REDPANDA_LICENSE_FILEPATH: /etc/redpanda/redpanda.license
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["redpanda:9093"]
          schemaRegistry:
            enabled: true
            urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
        connect:
          enabled: false
          clusters:
            - name: datagen
              url: http://redpanda-connect:8083
        login:
          enabled: false
          jwtSecret: change-this-to-something-secret
          useSecureCookies: false
          plain:
            enabled: true
            credentials:
            - username: "jdoe"
              password: "jdoe"
            - username: "john"
              password: "some-secret-password"
        enterprise:
          rbac:
            enabled: false
            roleBindingsFilepath: /tmp/role-bindings.yml
      CONSOLE_ROLEBINDINGS_CONFIG_FILE: |
        roleBindings:
        - metadata:
            name: Platform Ops
          subjects:
            - kind: user
              provider: Plain
              name: jdoe
          roleName: admin
        - metadata:
            name: Software Engineers
          subjects:
            - kind: user
              provider: Plain
              name: john
          roleName: editor
    ports:
      - 9001:8080
    depends_on:
      - redpanda
  kafdrop:
    container_name: kafdrop
    image: obsidiandynamics/kafdrop:4.0.2
    profiles:
      - messaging-redpanda-kafdrop
    depends_on:
      - redpanda
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: "redpanda:9093"
      JVM_OPTS: "-Xms32M -Xmx64M"
      SERVER_SERVLET_CONTEXTPATH: "/"
  jaeger:
    container_name: jaeger
    image: jaegertracing/jaeger:2.1.0
    profiles:
      - full
      - monitoring
    volumes:
      - './jaeger/jaeger-ui.json:/etc/jaeger/jaeger-ui.json'
    #command: '--query.ui-config /etc/jaeger/jaeger-ui.json'
    environment:
      - METRICS_STORAGE_TYPE=prometheus
      - 'PROMETHEUS_SERVER_URL=http://prometheus:9090'
      - >-
        PROMETHEUS_QUERY_SUPPORT_SPANMETRICS_CONNECTOR=${PROMETHEUS_QUERY_SUPPORT_SPANMETRICS_CONNECTOR:-true}
      - 'PROMETHEUS_QUERY_NAMESPACE=${PROMETHEUS_QUERY_NAMESPACE:-}'
      - 'PROMETHEUS_QUERY_DURATION_UNIT=${PROMETHEUS_QUERY_DURATION_UNIT:-}'
      - PROMETHEUS_QUERY_NORMALIZE_CALLS=true
      - PROMETHEUS_QUERY_NORMALIZE_DURATION=true
    ports:
      - 16686:16686
      - 14268:14268
      - 14269:14269
      - 14250:14250
      - 5778:5778
      - 9411:9411
  otel_collector:
    container_name: opentelemetry-collector
    image: otel/opentelemetry-collector-contrib:0.116.1
    profiles:
      - full
      - monitoring
    volumes:
      - >-
        ./opentelemetry/otel-collector-config.yaml:/etc/otelcol/otel-collector-config.yaml:z
    command: '--config=/etc/otelcol/otel-collector-config.yaml'
    ports:
      - '8889:8889'
      - '13133:13133'
      - '4317:4317'
      - '4318:4318'
      - '15680:55680'
    depends_on:
      - jaeger
  microsim:
    container_name: microsim
    image: 'yurishkuro/microsim:0.3.0'
    profiles:
      - microsim
    command: '-j http://otel_collector:14278/api/traces -d 24h -s 500ms'
    depends_on:
      - otel_collector
  prometheus:
    container_name: prometheus
    image: prom/prometheus:v3.1.0
    profiles:
      - full
      - monitoring
    volumes:
      - './prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:z'
    ports:
      - '19090:9090'
  grafana:
    container_name: grafana
    image: grafana/grafana:11.4.0
    profiles:
      - full
      - monitoring
    volumes:
      - './grafana/grafana.ini:/etc/grafana/grafana.ini:z'
      - >-
       ./grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yaml:z
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=postgres
      - GF_DATABASE_PASSWORD=postgres
    ports:
      - '3000:3000'
    depends_on:
      postgres:
        condition: service_healthy
  init-tempo:
    image: grafana/tempo:2.7.0
    profiles:
      - init
    user: root
    entrypoint:
      - "chown"
      - "10001:10001"
      - "/var/tempo"
    volumes:
      - ./tempo/data:/var/tempo
  tempo:
    container_name: tempo
    image: grafana/tempo:2.7.0
    profiles:
      - full
      - monitoring
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo/tempo.yaml:/etc/tempo.yaml:z
      - ./tempo/data:/var/tempo:z
    ports:
      - "3200:3200" # tempo
      - "14317:4317" # otlp grpc
  loki:
    container_name: loki
    image: grafana/loki:3.2.0
    profiles:
      - full
      - monitoring
    ports:
      - "3100:3100"
    volumes:
      - ./loki/local-config.yaml:/etc/loki/local-config.yaml:z
    command: -config.file=/etc/loki/local-config.yaml
  elasticsearch:
    container_name: elasticsearch
    image: docker.io/elastic/elasticsearch:8.16.2
    profiles:
      - full
      - elk
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      discovery.type: "single-node"
      xpack.security.enabled: "false"
      cluster.routing.allocation.disk.threshold_enabled: "false"
  logstash:
    container_name: logstash
    image: docker.io/elastic/logstash:8.16.2
    profiles:
      - full
      - elk
    environment:
      monitoring.elasticsearch.hosts: http://elasticsearch-host:9200
    volumes:
      - ./logstash/pipelines:/usr/share/logstash/pipeline:z
    ports:
      - "12201:12201/udp"
      - "5000:5000"
      - "9600:9600"
    depends_on:
      - elasticsearch
  kibana:
    container_name: kibana
    image: docker.io/elastic/kibana:8.16.2
    environment:
      xpack.monitoring.collection.enabled: "true"
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: d121d3beceec14b5c20ee815cd2c9337
      XPACK_REPORTING_ENCRIPTIONKEY: 3739d02dd358ccb3915cbbd89a6f5150
      XPACK_SECURITY_ENCRIPTIONKEY: 5c4eaf77fdde275944bb1a876a9af87a
    profiles:
      - full
      - elk
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
  wiremock-studio:
    container_name: wiremock-studio
    image: jizuzquiza/wiremock-studio:2.32.0-18
    profiles:
      - full
      - mock-server
    volumes:
      - ./wiremock-data-storage:/home/wiremock:z
    ports:
    - 19000-19010:9000-9010
  castlemock:
    container_name: castlemock
    profiles:
      - full
      - mock-server
    image: castlemock/castlemock:v1.63
    ports:
      - 19020:8080
volumes:
    mysql_data: {}
    redpanda_data: {}
